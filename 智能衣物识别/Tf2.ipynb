{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#数据集导入\n",
    "(train_image,train_label), (test_image,test_label) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "#归一化\n",
    "train_image = train_image/255\n",
    "test_image = test_image/255\n",
    "#独热\n",
    "train_label_onehot = tf.keras.utils.to_categorical(train_label)\n",
    "test_label_onehot = tf.keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
      "  0.         0.00392157 0.01568627 0.         0.         0.\n",
      "  0.         0.00392157 0.00392157 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
      "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
      "  0.01568627 0.         0.         0.01176471]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
      "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
      "  0.         0.04705882 0.03921569 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
      "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
      "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.00392157\n",
      "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
      "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
      "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
      "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
      "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
      "  0.48235294 0.76862745 0.89803922 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
      "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
      "  0.8745098  0.96078431 0.67843137 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
      "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
      "  0.8627451  0.95294118 0.79215686 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.01176471 0.\n",
      "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
      "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
      "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02352941 0.\n",
      "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
      "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
      "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01568627 0.         0.\n",
      "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
      "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
      "  0.85098039 0.81960784 0.36078431 0.        ]\n",
      " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
      "  0.00784314 0.         0.         0.         0.         0.\n",
      "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
      "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
      "  0.85490196 1.         0.30196078 0.        ]\n",
      " [0.         0.01176471 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
      "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
      "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
      "  0.87843137 0.95686275 0.62352941 0.        ]\n",
      " [0.         0.         0.         0.         0.07058824 0.17254902\n",
      "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
      "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
      "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
      "  0.91372549 0.93333333 0.84313725 0.        ]\n",
      " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
      "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
      "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
      "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
      "  0.8627451  0.90980392 0.96470588 0.        ]\n",
      " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
      "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
      "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
      "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
      "  0.87058824 0.89411765 0.88235294 0.        ]\n",
      " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
      "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
      "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
      "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
      "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
      " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
      "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
      "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
      "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
      "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
      " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
      "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
      "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
      "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
      "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
      " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
      "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
      "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
      "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
      "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
      " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
      "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
      "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
      "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
      "  0.75294118 0.84705882 0.66666667 0.        ]\n",
      " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
      "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
      "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
      "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
      "  0.38823529 0.22745098 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
      "  0.1372549  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00392157 0.         0.         0.05098039 0.28627451 0.\n",
      "  0.         0.00392157 0.01568627 0.         0.         0.\n",
      "  0.         0.00392157 0.00392157 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01176471 0.         0.14117647 0.53333333 0.49803922 0.24313725\n",
      "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
      "  0.01568627 0.         0.         0.01176471]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.02352941 0.         0.4        0.8        0.69019608 0.5254902\n",
      "  0.56470588 0.48235294 0.09019608 0.         0.         0.\n",
      "  0.         0.04705882 0.03921569 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.60784314 0.9254902  0.81176471 0.69803922\n",
      "  0.41960784 0.61176471 0.63137255 0.42745098 0.25098039 0.09019608\n",
      "  0.30196078 0.50980392 0.28235294 0.05882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.00392157\n",
      "  0.         0.27058824 0.81176471 0.8745098  0.85490196 0.84705882\n",
      "  0.84705882 0.63921569 0.49803922 0.4745098  0.47843137 0.57254902\n",
      "  0.55294118 0.34509804 0.6745098  0.25882353]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
      "  0.         0.78431373 0.90980392 0.90980392 0.91372549 0.89803922\n",
      "  0.8745098  0.8745098  0.84313725 0.83529412 0.64313725 0.49803922\n",
      "  0.48235294 0.76862745 0.89803922 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.71764706 0.88235294 0.84705882 0.8745098  0.89411765\n",
      "  0.92156863 0.89019608 0.87843137 0.87058824 0.87843137 0.86666667\n",
      "  0.8745098  0.96078431 0.67843137 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.75686275 0.89411765 0.85490196 0.83529412 0.77647059\n",
      "  0.70588235 0.83137255 0.82352941 0.82745098 0.83529412 0.8745098\n",
      "  0.8627451  0.95294118 0.79215686 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.00392157 0.01176471 0.\n",
      "  0.04705882 0.85882353 0.8627451  0.83137255 0.85490196 0.75294118\n",
      "  0.6627451  0.89019608 0.81568627 0.85490196 0.87843137 0.83137255\n",
      "  0.88627451 0.77254902 0.81960784 0.20392157]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.02352941 0.\n",
      "  0.38823529 0.95686275 0.87058824 0.8627451  0.85490196 0.79607843\n",
      "  0.77647059 0.86666667 0.84313725 0.83529412 0.87058824 0.8627451\n",
      "  0.96078431 0.46666667 0.65490196 0.21960784]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01568627 0.         0.\n",
      "  0.21568627 0.9254902  0.89411765 0.90196078 0.89411765 0.94117647\n",
      "  0.90980392 0.83529412 0.85490196 0.8745098  0.91764706 0.85098039\n",
      "  0.85098039 0.81960784 0.36078431 0.        ]\n",
      " [0.         0.         0.00392157 0.01568627 0.02352941 0.02745098\n",
      "  0.00784314 0.         0.         0.         0.         0.\n",
      "  0.92941176 0.88627451 0.85098039 0.8745098  0.87058824 0.85882353\n",
      "  0.87058824 0.86666667 0.84705882 0.8745098  0.89803922 0.84313725\n",
      "  0.85490196 1.         0.30196078 0.        ]\n",
      " [0.         0.01176471 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.24313725 0.56862745 0.8\n",
      "  0.89411765 0.81176471 0.83529412 0.86666667 0.85490196 0.81568627\n",
      "  0.82745098 0.85490196 0.87843137 0.8745098  0.85882353 0.84313725\n",
      "  0.87843137 0.95686275 0.62352941 0.        ]\n",
      " [0.         0.         0.         0.         0.07058824 0.17254902\n",
      "  0.32156863 0.41960784 0.74117647 0.89411765 0.8627451  0.87058824\n",
      "  0.85098039 0.88627451 0.78431373 0.80392157 0.82745098 0.90196078\n",
      "  0.87843137 0.91764706 0.69019608 0.7372549  0.98039216 0.97254902\n",
      "  0.91372549 0.93333333 0.84313725 0.        ]\n",
      " [0.         0.22352941 0.73333333 0.81568627 0.87843137 0.86666667\n",
      "  0.87843137 0.81568627 0.8        0.83921569 0.81568627 0.81960784\n",
      "  0.78431373 0.62352941 0.96078431 0.75686275 0.80784314 0.8745098\n",
      "  1.         1.         0.86666667 0.91764706 0.86666667 0.82745098\n",
      "  0.8627451  0.90980392 0.96470588 0.        ]\n",
      " [0.01176471 0.79215686 0.89411765 0.87843137 0.86666667 0.82745098\n",
      "  0.82745098 0.83921569 0.80392157 0.80392157 0.80392157 0.8627451\n",
      "  0.94117647 0.31372549 0.58823529 1.         0.89803922 0.86666667\n",
      "  0.7372549  0.60392157 0.74901961 0.82352941 0.8        0.81960784\n",
      "  0.87058824 0.89411765 0.88235294 0.        ]\n",
      " [0.38431373 0.91372549 0.77647059 0.82352941 0.87058824 0.89803922\n",
      "  0.89803922 0.91764706 0.97647059 0.8627451  0.76078431 0.84313725\n",
      "  0.85098039 0.94509804 0.25490196 0.28627451 0.41568627 0.45882353\n",
      "  0.65882353 0.85882353 0.86666667 0.84313725 0.85098039 0.8745098\n",
      "  0.8745098  0.87843137 0.89803922 0.11372549]\n",
      " [0.29411765 0.8        0.83137255 0.8        0.75686275 0.80392157\n",
      "  0.82745098 0.88235294 0.84705882 0.7254902  0.77254902 0.80784314\n",
      "  0.77647059 0.83529412 0.94117647 0.76470588 0.89019608 0.96078431\n",
      "  0.9372549  0.8745098  0.85490196 0.83137255 0.81960784 0.87058824\n",
      "  0.8627451  0.86666667 0.90196078 0.2627451 ]\n",
      " [0.18823529 0.79607843 0.71764706 0.76078431 0.83529412 0.77254902\n",
      "  0.7254902  0.74509804 0.76078431 0.75294118 0.79215686 0.83921569\n",
      "  0.85882353 0.86666667 0.8627451  0.9254902  0.88235294 0.84705882\n",
      "  0.78039216 0.80784314 0.72941176 0.70980392 0.69411765 0.6745098\n",
      "  0.70980392 0.80392157 0.80784314 0.45098039]\n",
      " [0.         0.47843137 0.85882353 0.75686275 0.70196078 0.67058824\n",
      "  0.71764706 0.76862745 0.8        0.82352941 0.83529412 0.81176471\n",
      "  0.82745098 0.82352941 0.78431373 0.76862745 0.76078431 0.74901961\n",
      "  0.76470588 0.74901961 0.77647059 0.75294118 0.69019608 0.61176471\n",
      "  0.65490196 0.69411765 0.82352941 0.36078431]\n",
      " [0.         0.         0.29019608 0.74117647 0.83137255 0.74901961\n",
      "  0.68627451 0.6745098  0.68627451 0.70980392 0.7254902  0.7372549\n",
      "  0.74117647 0.7372549  0.75686275 0.77647059 0.8        0.81960784\n",
      "  0.82352941 0.82352941 0.82745098 0.7372549  0.7372549  0.76078431\n",
      "  0.75294118 0.84705882 0.66666667 0.        ]\n",
      " [0.00784314 0.         0.         0.         0.25882353 0.78431373\n",
      "  0.87058824 0.92941176 0.9372549  0.94901961 0.96470588 0.95294118\n",
      "  0.95686275 0.86666667 0.8627451  0.75686275 0.74901961 0.70196078\n",
      "  0.71372549 0.71372549 0.70980392 0.69019608 0.65098039 0.65882353\n",
      "  0.38823529 0.22745098 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.15686275 0.23921569 0.17254902 0.28235294 0.16078431\n",
      "  0.1372549  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27238ab7188>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#显示数据集信息\n",
    "print(train_image.shape[0])\n",
    "print(train_label.shape)\n",
    "print(test_image.shape)\n",
    "print(test_label.shape)\n",
    "print(train_image[0])\n",
    "print(train_image[0])\n",
    "print(train_label_onehot)\n",
    "plt.imshow(train_image[1],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 203,530\n",
      "Trainable params: 203,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#建立网络\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "#模型信息\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.08627451 0.4627451  0.09411765\n",
      "  0.         0.         0.         0.         0.         0.18823529\n",
      "  0.34509804 0.01960784 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04705882 0.39215686 0.83137255 0.80392157\n",
      "  0.7254902  0.70196078 0.67843137 0.72941176 0.75686275 0.86666667\n",
      "  0.55686275 0.33333333 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.33333333 0.29803922 0.78039216\n",
      "  0.88235294 0.97254902 1.         0.93333333 0.88627451 0.61568627\n",
      "  0.26666667 0.31372549 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.35686275 0.27058824 0.35686275\n",
      "  0.78823529 0.85490196 0.88235294 0.81960784 0.61960784 0.23921569\n",
      "  0.36470588 0.28235294 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30980392 0.34901961 0.23921569\n",
      "  0.23137255 0.34117647 0.42352941 0.29411765 0.21960784 0.29803922\n",
      "  0.38039216 0.28627451 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29411765 0.34901961 0.31372549\n",
      "  0.31372549 0.2627451  0.24705882 0.28627451 0.3254902  0.31372549\n",
      "  0.37647059 0.28235294 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30196078 0.34509804 0.30196078\n",
      "  0.31372549 0.3254902  0.3254902  0.3254902  0.3254902  0.31764706\n",
      "  0.37254902 0.29803922 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34901961 0.37647059 0.31372549\n",
      "  0.3254902  0.31764706 0.32941176 0.33333333 0.33333333 0.33333333\n",
      "  0.38039216 0.32941176 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.36470588 0.38039216 0.31764706\n",
      "  0.33333333 0.32941176 0.33333333 0.34117647 0.34509804 0.32941176\n",
      "  0.38823529 0.34117647 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.37254902 0.34117647 0.32941176\n",
      "  0.34117647 0.34509804 0.33333333 0.34117647 0.34117647 0.32941176\n",
      "  0.36078431 0.34117647 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.38039216 0.34117647 0.34117647\n",
      "  0.33333333 0.34509804 0.34117647 0.34117647 0.34117647 0.34509804\n",
      "  0.33333333 0.41960784 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.06666667 0.39215686 0.34509804 0.34117647\n",
      "  0.34117647 0.34509804 0.34117647 0.34117647 0.33333333 0.34901961\n",
      "  0.30196078 0.4627451  0.03137255 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03921569 0.36470588 0.34117647 0.34117647\n",
      "  0.34117647 0.34117647 0.34117647 0.34509804 0.34117647 0.34901961\n",
      "  0.31372549 0.40392157 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03529412 0.37647059 0.34117647 0.34117647\n",
      "  0.34117647 0.34117647 0.34117647 0.34509804 0.34117647 0.34509804\n",
      "  0.34117647 0.40392157 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.04705882 0.37647059 0.33333333 0.34117647\n",
      "  0.34117647 0.34117647 0.33333333 0.34117647 0.34117647 0.34509804\n",
      "  0.34901961 0.39215686 0.00784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.07843137 0.37254902 0.32941176 0.34509804\n",
      "  0.33333333 0.34117647 0.34509804 0.34509804 0.34509804 0.34901961\n",
      "  0.34509804 0.38823529 0.03137255 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.08235294 0.37647059 0.33333333 0.34117647\n",
      "  0.33333333 0.34509804 0.34509804 0.34509804 0.34509804 0.34901961\n",
      "  0.34901961 0.38823529 0.03921569 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09411765 0.37647059 0.33333333 0.34117647\n",
      "  0.33333333 0.34117647 0.34509804 0.34509804 0.34901961 0.34509804\n",
      "  0.35686275 0.4        0.05490196 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.09803922 0.36470588 0.32941176 0.34509804\n",
      "  0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.34901961\n",
      "  0.35686275 0.40392157 0.11372549 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.11764706 0.37254902 0.33333333 0.34509804\n",
      "  0.34509804 0.34117647 0.34117647 0.34117647 0.34117647 0.34901961\n",
      "  0.34509804 0.4        0.14509804 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.13333333 0.37647059 0.34509804 0.34117647\n",
      "  0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.33333333\n",
      "  0.33333333 0.38039216 0.14901961 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.15686275 0.37647059 0.34117647 0.33333333\n",
      "  0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.33333333\n",
      "  0.32941176 0.36078431 0.19215686 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.18039216 0.37254902 0.3254902  0.32941176\n",
      "  0.34117647 0.34117647 0.34117647 0.34117647 0.34117647 0.34117647\n",
      "  0.32941176 0.34117647 0.32941176 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28235294 0.37254902 0.33333333 0.32941176\n",
      "  0.33333333 0.34509804 0.34117647 0.34117647 0.34901961 0.34117647\n",
      "  0.33333333 0.3254902  0.24705882 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.25098039 0.39215686 0.32941176 0.34117647\n",
      "  0.34509804 0.33333333 0.34509804 0.34509804 0.32941176 0.34117647\n",
      "  0.3254902  0.37254902 0.20784314 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.03921569 0.4        0.39215686 0.35686275\n",
      "  0.35686275 0.34901961 0.33333333 0.32941176 0.32941176 0.34117647\n",
      "  0.42352941 0.41568627 0.05490196 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.03137255 0.28627451 0.36470588\n",
      "  0.40784314 0.41960784 0.40392157 0.40392157 0.41568627 0.4\n",
      "  0.29411765 0.03921569 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.00392157 0.         0.         0.\n",
      "  0.07058824 0.16470588 0.22352941 0.21960784 0.1254902  0.03137255\n",
      "  0.         0.         0.00392157 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#随机进行反色处理\n",
    "train_image_inverted = train_image\n",
    "for i in range(0,train_image_inverted.shape[0]-1):\n",
    "    if(random.randint(0,1)):\n",
    "        train_image_inverted[i] = 1 - train_image_inverted[i]\n",
    "print(train_image_inverted[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.3008 - acc: 0.8882\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2938 - acc: 0.8915\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2854 - acc: 0.8948\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.2802 - acc: 0.8963\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2724 - acc: 0.9002\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2667 - acc: 0.9012\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2618 - acc: 0.9022\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2573 - acc: 0.9033\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2538 - acc: 0.9055\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2486 - acc: 0.9068\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2451 - acc: 0.9091\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2415 - acc: 0.9101\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2392 - acc: 0.9113\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2328 - acc: 0.9141\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2300 - acc: 0.9140\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2262 - acc: 0.9165\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.2236 - acc: 0.9176\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2208 - acc: 0.9175\n",
      "Epoch 19/100\n",
      "   32/60000 [..............................] - ETA: 9s - loss: 0.1364 - acc: 0.9375"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f5b687de8bee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#编译并运行\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_image_inverted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label_onehot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mE:\\anacoda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#编译并运行\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n",
    "history = model.fit(train_image_inverted, train_label_onehot, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试\n",
    "model.evaluate(test_image,test_label_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取并转换为28行28列数组,接着进行归一化\n",
    "input = Image.open('d:\\\\3.jpg')\n",
    "input = input.resize((28,28),resample = Image.BILINEAR)\n",
    "input_gray = input.convert('L')\n",
    "plt.imshow(input_gray,cmap = 'gray')\n",
    "to_predict = np.asarray(input_gray)\n",
    "to_predict = to_predict/255\n",
    "\n",
    "#数组升维，接着进行预测\n",
    "to_predict = np.expand_dims(to_predict, axis=0)\n",
    "tag_onehot = model.predict(to_predict)\n",
    "possibility = 0\n",
    "for i in range(0,9):\n",
    "    if possibility < tag_onehot[0][i]:\n",
    "        possibility = tag_onehot[0][i]\n",
    "        flag = i\n",
    "dict={\n",
    "    0:'T-Shirt/Top',\n",
    "    1:'Trouser',\n",
    "    2:'Pullover',\n",
    "    3:'Dress',\n",
    "    4:'Coat',\n",
    "    5:'Sandals',\n",
    "    6:'Shirt',\n",
    "    7:'Sneaker',\n",
    "    8:'Bag',\n",
    "    9:'Ankle boots'\n",
    "}\n",
    "print(flag)\n",
    "print(dict[flag], end=\" \")\n",
    "print('with a', end=\" \")\n",
    "print(possibility, end=\" \")\n",
    "print('confidence')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
